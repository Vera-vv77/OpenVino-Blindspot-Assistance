# OpenVino Blindspot Assistance
:idprefix:
:idseparator: -
:sectanchors:
:sectlinks:
:sectnumlevels: 6
:sectnums:
:toc: macro
:toclevels: 6
:toc-title: Table of Contents

https://travis-ci.org/incluit/OpenVino-Blindspot-Assistance[image:https://travis-ci.org/incluit/OpenVino-Blindspot-Assistance.svg?branch=master[Build
Status]]
https://sonarcloud.io/dashboard?id=incluit_OpenVino-Blindspot-Assistance[image:https://sonarcloud.io/api/project_badges/measure?project=incluit_OpenVino-Blindspot-Assistance&metric=alert_status[Sonarcloud
Status]]


toc::[]

== Introduction

This project consists on showcasing the advantages of the Intel's OpenVINO toolkit. We will develop a __Blindspot Assistance__ case scenario, where we will define a blindspot area to detect vehicles and pedestrians, up to 4 cameras simultaneously. For that, we will use the OpenVINO toolkit and OpenCV.

=== [Optional] AWS (In Progress)

We also plan to send the data through MQTT using AWS IoT-Core. Using AWS may incur in a cost, so this will also be optional for you to run with/without it.

== Bussines Logic

Using OpenVino's model detection we can easily detect pedestrians and vehicles with great accuracy. We are currently using a pedestrian and vehicle detection model that is included with OpenVino out-of-the-box:

. pedestrian-and-vehicle-detector-adas-0001

== Prerequisites

To run the application in this tutorial, the OpenVINO™ toolkit (Release R2 or greater) and its dependencies must already be installed and verified using the included demos. Installation instructions may be found at: https://software.intel.com/en-us/articles/OpenVINO-Install-Linux

If to be used, any optional hardware must also be installed and verified including:

* USB camera - Standard USB Video Class (UVC) camera.

* Intel® Core™ CPU with integrated graphics.

* VPU - USB Intel® Movidius™ Neural Compute Stick and what is being referred to as "Myriad"

A summary of what is needed:

=== Hardware

* Target and development platforms meeting the requirements described in the "System Requirements" section of the OpenVINO™ toolkit documentation which may be found at: https://software.intel.com/en-us/openvino-toolkit

**Note**: While writing this tutorial, an Intel® i7-8550U with Intel® HD graphics 520 GPU was used as both the development and target platform.

* Optional:

** Intel® Movidius™ Neural Compute Stick

** USB UVC camera

** Intel® Core™ CPU with integrated graphics.

=== Software

* OpenVINO™ toolkit supported Linux operating system. This tutorial was run on 64-bit Ubuntu 18.04.3 LTS updated to kernel 4.15.0-66 following the OpenVINO™ toolkit installation instructions.

* The latest OpenVINO™ toolkit installed and verified. Supported versions +2019 R2. (Lastest version supported 2019 R3.1).

* Git(git) for downloading from the GitHub repository.

=== Checks

By now you should have completed the Linux installation guide for the OpenVINO™ toolkit, however before continuing, please ensure:

* That after installing the OpenVINO™ toolkit you have run the supplied demo samples 

* If you have and intend to use a GPU: You have installed and tested the GPU drivers 

* If you have and intend to use a USB camera: You have connected and tested the USB camera 

* If you have and intend to use a Myriad: You have connected and tested the USB Intel® Movidius™ Neural Compute Stick

* That your development platform is connected to a network and has Internet access. To download all the files for this tutorial, you will need to access GitHub on the Internet. 

== Building

=== Basic Build

**1.** Clone the repository at desired location:

[source,bash]
----
git clone https://github.com/incluit/OpenVino-Blindspot-Assistance.git
----

**2.** The first step is to configure the build environment for the OpenCV toolkit by sourcing the "setupvars.sh" script:

[source,bash]
----
source  /opt/intel/openvino/bin/setupvars.sh
----

**3.** Change to the BlindspotAssistance folder:

[source,bash]
----
cd OpenVino-Blindspot-Assistance/BlindspotAssistance/
----

**4.** Download the necessary model

[source,bash]
----
bash scripts/download_models.sh
----

**5.** Create a directory to build the tutorial in and change to it.

[source,bash]
----
mkdir build
cd build
----

**6.** Before running each of the following sections, be sure to source the helper script. That will make it easier to use environment variables instead of long names to the models:

[source,bash]
----
source ../scripts/setupenv.sh
----

**7.** Compile

[source,bash]
----
cmake -DCMAKE_BUILD_TYPE=Release ../
make
----

**8.** Move to the executable’s dir:

[source,bash]
----
cd intel64/Release
----


=== Build with Docker

==== Prerequisites

* Docker. To install on Ubuntu, run:

[source,bash]
----
sudo snap install docker

sudo groupadd docker

sudo usermod -aG docker $USER
----

==== Installation

**1.** Clone the repository at desired location:

[source,bash]
----
git clone https://github.com/incluit/OpenVino-Blindspot-Assistance.git
----

**2.** Change to the top git repository:

[source,bash]
----
cd OpenVino-Blindspot-Assistance
----

**3.** Build the docker:
[source,bash]
----
make docker-build
----

=== Build in DockerHub
The master brach of this repository is built in Docker Hub:
https://hub.docker.com/repository/docker/openvinoincluit/blindspot-openvino/general

=== MQTT Broker Instalation

----
docker run -d --rm --name broker -p 1883:1883 eclipse-mosquitto
----

== Usage

=== Run

Run the application with the `-h` option to see the usage message:

[source,bash]
----
./blindspot-assistance -h
----

Options:
[source,bash]
----
    -h                           Print a usage message
    -m "<path>"                  Required. Path to an .xml file with a trained model.
      -l "<absolute_path>"       Required for CPU custom layers. Absolute path to a shared library with the kernel implementations
          Or
      -c "<absolute_path>"       Required for GPU custom kernels. Absolute path to an .xml file with the kernel descriptions
    -d "<device>"                Optional. Specify the target device for a network (the list of available devices is shown below). Default value is CPU. Use "-d HETERO:<comma-separated_devices_list>" format to specify HETERO plugin. The demo looks for a suitable plugin for a specified device.
    -nc                          Optional. Maximum number of processed camera inputs (web cameras)
    -bs                          Optional. Batch size for processing (the number of frames processed per infer request)
    -nireq                       Optional. Number of infer requests
    -n_iqs                       Optional. Frame queue size for input channels
    -fps_sp                      Optional. FPS measurement sampling period between timepoints in msec
    -n_sp                        Optional. Number of sampling periods
    -pc                          Optional. Enable per-layer performance report
    -t                           Optional. Probability threshold for detections
    -no_show                     Optional. Do not show processed video
    -no_show_d                   Optional. Optional. Do not show detected objects.
    -show_stats                  Optional. Enable statistics report
    -duplicate_num               Optional. Enable and specify the number of channels additionally copied from real sources
    -real_input_fps              Optional. Disable input frames caching for maximum throughput pipeline
    -i                           Optional. Specify full path to input video files
    -loop_video                  Optional. Enable playing video on a loop.
    -u                           Optional. List of monitors to show initially.
----

==== Examples

**1.** Using 4 video files, using CPU and GPU:

[source,bash]
----
./blindspot-assistance -m ../../../models/FP32/pedestrian-and-vehicle-detector-adas-0001.xml -d HETERO:CPU,GPU -i ../../../data/BlindspotFront.mp4 ../../../data/BlindspotLeft.mp4 ../../../data/BlindspotRear.mp4 ../../../data/BlindspotRight.mp4
----

**2.** Using 3 video files and 1 camera (`-nc 1`):

[source,bash]
----
./blindspot-assistance -m ../../../models/FP32/pedestrian-and-vehicle-detector-adas-0001.xml -d HETERO:CPU,GPU -i ../../../data/BlindspotFront.mp4 ../../../data/BlindspotLeft.mp4 ../../../data/BlindspotRear.mp4 -nc 1
----

**3.** Using 4 video files, showing statistics report (`-show_stats`):

[source,bash]
----
./blindspot-assistance -m ../../../models/FP32/pedestrian-and-vehicle-detector-adas-0001.xml -d HETERO:CPU,GPU -i ../../../data/BlindspotFront.mp4 ../../../data/BlindspotLeft.mp4 ../../../data/BlindspotRear.mp4 ../../../data/BlindspotRight.mp4 -show_stats
----

**4.** Using 4 video files, without video output (`-no_show`). Average performance is displayed on the console.

[source,bash]
----
./blindspot-assistance -m ../../../models/FP32/pedestrian-and-vehicle-detector-adas-0001.xml -d HETERO:CPU,GPU -i ../../../data/BlindspotFront.mp4 ../../../data/BlindspotLeft.mp4 ../../../data/BlindspotRear.mp4 ../../../data/BlindspotRight.mp4 -no_show
----

**5.** Using 4 video files, without detected objects (`-no_show_d`). 

[source,bash]
----
./blindspot-assistance -m ../../../models/FP32/pedestrian-and-vehicle-detector-adas-0001.xml -d HETERO:CPU,GPU -i ../../../data/BlindspotFront.mp4 ../../../data/BlindspotLeft.mp4 ../../../data/BlindspotRear.mp4 ../../../data/BlindspotRight.mp4 -no_show_d
----

**5.** Using 4 video files, changing the probability threshold for detections (`-t 0.1`). 

[source,bash]
----
./blindspot-assistance -m ../../../models/FP32/pedestrian-and-vehicle-detector-adas-0001.xml -d HETERO:CPU,GPU -i ../../../data/BlindspotFront.mp4 ../../../data/BlindspotLeft.mp4 ../../../data/BlindspotRear.mp4 ../../../data/BlindspotRight.mp4 -t 0.1
----

===== Other models

You can also experiment by using different face detection models, being the ones available up to now:

. person-vehicle-bike-detection-crossroad-0078
. person-vehicle-bike-detection-crossroad-1016

[source,bash]
----
./blindspot-assistance -m ../../../models/FP32/person-vehicle-bike-detection-crossroad-0078.xml -d HETERO:CPU,GPU -i ../../../data/BlindspotFront.mp4 ../../../data/BlindspotLeft.mp4 ../../../data/BlindspotRear.mp4 ../../../data/BlindspotRight.mp4
----

[source,bash]
----
./blindspot-assistance -m ../../../models/FP32/person-vehicle-bike-detection-crossroad-1016.xml -d HETERO:CPU,GPU -i ../../../data/BlindspotFront.mp4 ../../../data/BlindspotLeft.mp4 ../../../data/BlindspotRear.mp4 ../../../data/BlindspotRight.mp4
----

==== Run in Docker
**1.** Run the docker:

[source,bash]
----
make docker-run
----

**2.** Run the example inside the Docker or try the detailed examples in Usage.
[source,bash]
----
make run
----

**3** If you get out of the Docker, you can run it again:
[source,bash]
----
docker start blindspotcont
docker exec -it blindspotcont /bin/bash
---- 


== Troubleshooting

**1.** If you receive the following message inside the Docker:
[source,bash]
----
Gtk-WARNING **: 13:01:52.097: cannot open display: :0
----

Go outside the Docker container, run:
[source,bash]
----
xhost +
----
Enter the Docker container and run it again.